<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://luckyle.top').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="Python网络爬虫之requests模块什么是requests模块​        request模块是python中原生的基于网络请求的模块，其主要作用是用来模拟浏览器发起请求。功能强大，用法简洁高效。在爬虫领域中占着半壁江山的地位。 为什么要使用request模块​        因为在使用urllib模块的时候，会有诸多不便之处，总结如下：手动处理url编码，手动处理post请求参数，处理">
<meta name="keywords" content="Spider">
<meta property="og:type" content="article">
<meta property="og:title" content="Python网络爬虫之Requests模块">
<meta property="og:url" content="https:&#x2F;&#x2F;luckyle.top&#x2F;Spider&#x2F;Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B9%8BRequests%E6%A8%A1%E5%9D%97&#x2F;index.html">
<meta property="og:site_name" content="可乐">
<meta property="og:description" content="Python网络爬虫之requests模块什么是requests模块​        request模块是python中原生的基于网络请求的模块，其主要作用是用来模拟浏览器发起请求。功能强大，用法简洁高效。在爬虫领域中占着半壁江山的地位。 为什么要使用request模块​        因为在使用urllib模块的时候，会有诸多不便之处，总结如下：手动处理url编码，手动处理post请求参数，处理">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-05-05T16:42:26.001Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://luckyle.top/Spider/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B9%8BRequests%E6%A8%A1%E5%9D%97/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Python网络爬虫之Requests模块 | 可乐</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="可乐" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">可乐</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">“我在寻找丢失的记忆”“说人话!”“我在复习”</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">12</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">12</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">37</span></a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://luckyle.top/Spider/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B9%8BRequests%E6%A8%A1%E5%9D%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/tx.jpg">
      <meta itemprop="name" content="杜家乐">
      <meta itemprop="description" content="Progress is not created by contented people..">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="可乐">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Python网络爬虫之Requests模块
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-03 19:34:34" itemprop="dateCreated datePublished" datetime="2020-02-03T19:34:34+08:00">2020-02-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-06 00:42:26" itemprop="dateModified" datetime="2020-05-06T00:42:26+08:00">2020-05-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spider/" itemprop="url" rel="index">
                    <span itemprop="name">Spider</span>
                  </a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>28k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>25 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="Python网络爬虫之requests模块"><a href="#Python网络爬虫之requests模块" class="headerlink" title="Python网络爬虫之requests模块"></a>Python网络爬虫之requests模块</h3><h6 id="什么是requests模块"><a href="#什么是requests模块" class="headerlink" title="什么是requests模块"></a>什么是requests模块</h6><p>​        request模块是python中原生的基于网络请求的模块，其主要作用是用来模拟浏览器发起请求。功能强大，用法简洁高效。在爬虫领域中占着半壁江山的地位。</p>
<h6 id="为什么要使用request模块"><a href="#为什么要使用request模块" class="headerlink" title="为什么要使用request模块"></a>为什么要使用request模块</h6><p>​        因为在使用urllib模块的时候，会有诸多不便之处，总结如下：手动处理url编码，手动处理post请求参数，处理cookie和代理操作频繁。使用request模块：自动处理url编码，自动处理post请求参数，简化cookie和代理操作  <a id="more"></a></p>
<h6 id="如何使用request模块"><a href="#如何使用request模块" class="headerlink" title="如何使用request模块"></a>如何使用request模块</h6><p>安装</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure>

<p>使用流程</p>
<p>指定url—-&gt;基于requests模块发起请求——-&gt;获取响应对象中的数据值——&gt;持久化存储</p>
<h6 id="request发送请求"><a href="#request发送请求" class="headerlink" title="request发送请求"></a>request发送请求</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>requests的get请求:</span><br><span class="line"><span class="comment"># requests的简单get请求</span></span><br><span class="line"><span class="comment"># requests.get + headers</span></span><br><span class="line"><span class="comment"># requests.get + headers + params</span></span><br><span class="line"><span class="comment"># requests.get + headers + params + proxy</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'...'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">	<span class="string">"User-Agent"</span>:<span class="string">'...'</span></span><br><span class="line">&#125;</span><br><span class="line">params = &#123;</span><br><span class="line">	<span class="string">'key'</span>: <span class="string">'value'</span></span><br><span class="line">&#125;</span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:8080'</span></span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'http://127.0.0.1:8899'</span></span><br><span class="line">&#125;</span><br><span class="line">res = requests.get(url=url, headers=headers, params=params, proxies=proxies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代理:</span></span><br><span class="line">	透明代理:</span><br><span class="line">	匿名代理:</span><br><span class="line">	高匿代理:</span><br><span class="line">        </span><br><span class="line"><span class="comment">#第一种: 反爬机制与反反爬策略</span></span><br><span class="line">反爬机制: UA检测</span><br><span class="line">反反爬策略: UA伪装</span><br><span class="line">    </span><br><span class="line"><span class="number">3.</span>响应数据</span><br><span class="line"><span class="comment"># 获取响应数据内容:</span></span><br><span class="line">		res.text		获取HTML文本</span><br><span class="line">		res.content		获取二进制流</span><br><span class="line">		res.json()		获取json数据</span><br><span class="line"><span class="comment"># 响应数据的属性:</span></span><br><span class="line">	    res_code = res.status_code  <span class="comment"># 响应状态码(*)</span></span><br><span class="line">        res_headers = res.headers  <span class="comment"># 响应头信息</span></span><br><span class="line">        res_url = res.url  <span class="comment"># 此响应对应的请求url</span></span><br><span class="line">        res_cookie = res.cookies  <span class="comment"># 响应的cookies(*)</span></span><br><span class="line">        res_history = res.history  <span class="comment"># 请求历史</span></span><br></pre></td></tr></table></figure>

<h6 id="requests高阶用法"><a href="#requests高阶用法" class="headerlink" title="requests高阶用法"></a>requests高阶用法</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>requests上传文件操作</span><br><span class="line"><span class="number">2.</span>会话维持： Session对象</span><br><span class="line"><span class="number">3.</span>设置超时事件：timeout,请求<span class="number">5</span>秒内没有返回响应，则抛出异常</span><br><span class="line"><span class="number">4.</span>Prepare requests:构建requests对象，可以放入队列中实现爬取队列调度</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>requests上传文件操作</span><br><span class="line">	files=&#123;<span class="string">'file'</span>:open(<span class="string">'filename'</span>,<span class="string">'rb'</span>)&#125;</span><br><span class="line">    res=requests.post(url=url,files=files)</span><br><span class="line">    </span><br><span class="line"><span class="number">2.</span>会话维持  Session对象</span><br><span class="line">	<span class="keyword">from</span> requests <span class="keyword">import</span> Session</span><br><span class="line">    session=Session()</span><br><span class="line">    res=requests.get(url=url,headers=headers)</span><br><span class="line"><span class="number">3.</span>设置超时时间：timeout,请求<span class="number">5</span>秒内没有返回相应，则抛出异常</span><br><span class="line">	res=requests.get(url=url,headers=headers,timeout=<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line"><span class="number">4.</span>Prepare Request: 构建request对象, 可以放入队列中实现爬取队列调度</span><br><span class="line">        <span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br><span class="line">        url = <span class="string">'....'</span></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'wd'</span>: <span class="string">'spiderman'</span></span><br><span class="line">        &#125;</span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'...'</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># 1.实话session对象</span></span><br><span class="line">        session = Session()</span><br><span class="line">        <span class="comment"># 2.构建request对象, 传入必要参数</span></span><br><span class="line">        req = Request(<span class="string">'POST'</span>, url, data=data, headers=headers)</span><br><span class="line">        req = Request(<span class="string">'GET'</span>, url, params=params, headers=headers)</span><br><span class="line">        <span class="comment"># 3.应用prepared_request方法将request对象转化为Prepared Request对象</span></span><br><span class="line">        prepared = session.prepare_request(req)</span><br><span class="line">        <span class="comment"># 4.利用session的send方法发送请求</span></span><br><span class="line">        res = session.send(prepared)</span><br></pre></td></tr></table></figure>



<h6 id="xpath解析库"><a href="#xpath解析库" class="headerlink" title="xpath解析库"></a>xpath解析库</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Xpath解析库介绍:</span></span><br><span class="line">	数据解析的过程中使用过正则表达式, 但正则表达式想要进准匹配难度较高, 一旦正则表达式书写错误, 匹配的数据也会出错.</span><br><span class="line">    网页由三部分组成: HTML, Css, JavaScript, HTML页面标签存在层级关系, 即DOM树, 在获取目标数据时可以根据网页层次关系定位标签, 在获取标签的文本或属性.</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment"># xpath解析库解析数据原理:</span></span><br><span class="line"><span class="number">1.</span> 根据网页DOM树定位节点标签</span><br><span class="line"><span class="number">2.</span> 获取节点标签的征文文本即与属性值</span><br><span class="line"></span><br><span class="line"><span class="comment"># xpath安装, 初体验 --&gt; 使用步骤:</span></span><br><span class="line"><span class="number">1.</span>xpath安装: pip install lxml</span><br><span class="line"><span class="number">2.</span>requests模块爬取糗事百科热门的标题:</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://www.qiushibaike.com/'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">res = requests.get(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line">tree = etree.HTML(res.text)</span><br><span class="line">title_lst = tree.xpath(<span class="string">'//ul/li/div/a/text()'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> title_lst:</span><br><span class="line">    print(item)</span><br><span class="line">    </span><br><span class="line"><span class="number">3.</span>xpath使用步骤:</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">tree = etree.HTML(res.text)</span><br><span class="line">tree = etree.parse(res.html, etree.HTMLParse())  <span class="comment"># 示例如下, 了解内容</span></span><br><span class="line"></span><br><span class="line">tag_or_attr = tree.xpath(<span class="string">'xpath表达式'</span>)</span><br><span class="line"></span><br><span class="line">**********************************************************</span><br><span class="line"></span><br><span class="line"><span class="comment"># xpath解析本地文件</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://www.qiushibaike.com/'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">res = requests.get(url=url, headers=headers)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'qb.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(res.text)</span><br><span class="line"></span><br><span class="line">tree = etree.parse(<span class="string">'./qb.html'</span>, etree.HTMLParser())</span><br><span class="line">title_lst = tree.xpath(<span class="string">'//ul/li/div/a/text()'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> title_lst:</span><br><span class="line">    print(item)</span><br><span class="line">    </span><br><span class="line"> <span class="comment"># xpath语法:</span></span><br><span class="line"><span class="number">1.</span>常用规则:</span><br><span class="line">      <span class="number">1.</span>  nodename:		  节点名定位</span><br><span class="line">      <span class="number">2.</span>  //:			  从当前节点选取子孙节点</span><br><span class="line">      <span class="number">3.</span>  /:			  从当前节点选取直接子节点</span><br><span class="line">      <span class="number">4.</span>  nodename[@attribute=<span class="string">"..."</span>]  根据属性定位标签</span><br><span class="line">      <span class="number">5.</span>  @attributename:  获取属性 </span><br><span class="line">      <span class="number">6.</span>  text():		   获取文本</span><br><span class="line">   </span><br><span class="line"><span class="number">2.</span>属性匹配两种情况: 多属性匹配 &amp;  单属性多值匹配  </span><br><span class="line">     <span class="number">2.2</span> 多属性匹配</span><br><span class="line">    示例: tree.xpath(<span class="string">'//div[@class="item" and @name="test"]/text()'</span>) 	</span><br><span class="line">    <span class="number">2.1</span> 单属性多值匹配</span><br><span class="line">    示例: tree.xpath(<span class="string">'//div[contains(@class, "dc")]/text()'</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span>按序选择:</span><br><span class="line">    <span class="number">3.1</span> 索引定位: 从<span class="number">1</span>开始</span><br><span class="line">    <span class="number">3.2</span> last()函数</span><br><span class="line">    <span class="number">3.3</span> position()函数</span><br></pre></td></tr></table></figure>

<h6 id="BeautifulSoup解析"><a href="#BeautifulSoup解析" class="headerlink" title="BeautifulSoup解析"></a>BeautifulSoup解析</h6><p>BeautifulSoup也是一个解析库<br>    BS解析数据是依赖解析器的, BS支持的解析器有html.parser, lxml, xml, html5lib等, 其中lxml解析器解析速度快, 容错能力强.<br>BS现阶段应用的解析器多数是lxml</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BeautifulSoup 使用步骤:</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(res.text, <span class="string">'lxml'</span>)</span><br><span class="line">tag = soup.select(<span class="string">"CSS选择器表达式"</span>)   <span class="comment"># 返回一个列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CSS选择器:</span></span><br><span class="line"><span class="number">1.</span>根据节点名及节点层次关系定位标签: 标签选择器  &amp;  层级选择器</span><br><span class="line">soup.select(<span class="string">'title'</span>)</span><br><span class="line">soup.select(<span class="string">'div &gt; ul &gt; li'</span>)   <span class="comment"># 单层级选择器</span></span><br><span class="line">soup.select(<span class="string">'div li'</span>)  <span class="comment"># 多层级选择器</span></span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>根据节点的<span class="class"><span class="keyword">class</span>属性定位标签:</span> <span class="class"><span class="keyword">class</span>选择器</span></span><br><span class="line"><span class="class"><span class="title">soup</span>.<span class="title">select</span><span class="params">(<span class="string">'.panel'</span>)</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">3.根据<span class="title">id</span>属性定位标签:</span> id选择器</span><br><span class="line">soup.select(<span class="string">'#item'</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span>嵌套选择:</span><br><span class="line">ul_list = soup.select(<span class="string">'ul'</span>)</span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> ul_list:</span><br><span class="line">  print(ul.select(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取节点的文本或属性:</span></span><br><span class="line">tag_obj.string: 获取直接子文本--&gt;如果节点内有与直系文本平行的节点, 该方法拿到的是None</span><br><span class="line">tag_obj.get_text(): 获取子孙节点的所有文本</span><br><span class="line">tag_obj[<span class="string">'attribute'</span>]: 获取节点属性</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 练习示例:</span></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">    &lt;div class="panel"&gt;</span></span><br><span class="line"><span class="string">        &lt;div class="panel-heading"&gt;</span></span><br><span class="line"><span class="string">            &lt;h4&gt;BeautifulSoup练习&lt;/h4&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &lt;div class="panel-body"&gt;</span></span><br><span class="line"><span class="string">            &lt;ul class="list" id="list-1"&gt;</span></span><br><span class="line"><span class="string">                &lt;li class="element"&gt;第一个li标签&lt;/li&gt;</span></span><br><span class="line"><span class="string">                &lt;li class="element"&gt;第二个li标签&lt;/li&gt;</span></span><br><span class="line"><span class="string">                &lt;li class="element"&gt;第三个li标签&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;/ul&gt;</span></span><br><span class="line"><span class="string">            &lt;ul class="list list-small"&gt;</span></span><br><span class="line"><span class="string">                &lt;li class="element"&gt;one&lt;/li&gt;</span></span><br><span class="line"><span class="string">                &lt;li class="element"&gt;two&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;/ul&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;测试多层级选择器&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line"><span class="comment"># 1.根据节点名定位节点, 获取其文本</span></span><br><span class="line">h4 = soup.select(<span class="string">'h4'</span>)   <span class="comment"># 标签选择器</span></span><br><span class="line">print(h4[<span class="number">0</span>].get_text())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.根据class属性定位节点</span></span><br><span class="line">panel = soup.select(<span class="string">'.panel-heading'</span>)</span><br><span class="line">print(panel)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.根据id属性定位节点</span></span><br><span class="line">ul = soup.select(<span class="string">'#list-1'</span>)</span><br><span class="line">print(ul)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.嵌套选择</span></span><br><span class="line">ul_list = soup.select(<span class="string">'ul'</span>)</span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> ul_list:</span><br><span class="line">    li = ul.select(<span class="string">'li'</span>)</span><br><span class="line">    print(li)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 5.单层级选择器与多层级选择器</span></span><br><span class="line">li_list_single = soup.select(<span class="string">".panel-body &gt; ul &gt; li"</span>)</span><br><span class="line">li_list_multi = soup.select(<span class="string">".panel-body li"</span>)</span><br></pre></td></tr></table></figure>



<h6 id="代码展示"><a href="#代码展示" class="headerlink" title="代码展示"></a>代码展示</h6><p>需求：爬取搜狗指定词条搜索后的页面数据</p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import os</span><br><span class="line"><span class="meta">#指定搜索关键字</span></span><br><span class="line">word = input(<span class="string">'enter a word you want to search:'</span>)</span><br><span class="line"><span class="meta">#自定义请求头信息</span></span><br><span class="line">headers=&#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#指定url</span></span><br><span class="line">url = <span class="string">'https://www.sogou.com/web'</span></span><br><span class="line"><span class="meta">#封装get请求参数</span></span><br><span class="line">prams = &#123;</span><br><span class="line">    <span class="string">'query'</span>:word,</span><br><span class="line">    <span class="string">'ie'</span>:<span class="string">'utf-8'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#发起请求</span></span><br><span class="line">response = requests.<span class="keyword">get</span>(url=url,params=prams)</span><br><span class="line"></span><br><span class="line"><span class="meta">#获取响应数据</span></span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with open(<span class="string">'./sougou.html'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) as fp:</span><br><span class="line">    fp.write(page_text)</span><br></pre></td></tr></table></figure>

<h6 id="请求载体身份标识的伪装"><a href="#请求载体身份标识的伪装" class="headerlink" title="请求载体身份标识的伪装"></a>请求载体身份标识的伪装</h6><p>User-Agent:请求载体身份标识，通过浏览器发起的请求，请求载体为浏览器，则该请求的User-Agent为浏览器的身份标识。可以通过判断该值来获知该请求的载体究竟是基于那款浏览器还是基于爬虫程序。</p>
<p>反爬机制：某些门户网站会对访问该网站的请求中的User-Agent进行捕获和判断，如果该请求的UA为爬虫程序，则拒绝向该请求提供数据。</p>
<p>反反爬策略：将爬虫程序的UA伪装成某一款浏览器的身份标识</p>
<p>需求：requests带参的get请求</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#百度原 https://www.baidu.com/s?wd=python  </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span></span><br><span class="line">	&#125;</span><br><span class="line">url = <span class="string">'https://www.baidu.com/s'</span></span><br><span class="line">data = &#123;<span class="string">'wd'</span> : <span class="string">'Scrapy'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#url不需要编码     会自动拼接</span></span><br><span class="line"></span><br><span class="line">res=requests.get(url=url,headers=headers,params=data)</span><br><span class="line">print(res.url)</span><br></pre></td></tr></table></figure>

<p>爬取校花网图片案列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>&#125;</span><br><span class="line"><span class="comment">#获取网页地址</span></span><br><span class="line"><span class="comment">#http://www.xiaohuar.com/list-1-3.html</span></span><br><span class="line"><span class="comment">#http://www.xiaohuar.com/list-1-5.html</span></span><br><span class="line"><span class="comment"># 'http://www.xueshengmai.com/list-1-%d.html'</span></span><br><span class="line"></span><br><span class="line">url=<span class="string">'http://www.xueshengmai.com/list-1-%d.html'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):<span class="comment">#4表示要爬取4页图片，这里可根据需求做出修改。</span></span><br><span class="line">    temp=(url % i)</span><br><span class="line">    <span class="comment"># print(temp)</span></span><br><span class="line">    <span class="comment">#获取网页源码</span></span><br><span class="line">    response=requests.get(url=temp,headers=headers)</span><br><span class="line">    tree=etree.HTML(response.text)</span><br><span class="line">    <span class="keyword">for</span> td <span class="keyword">in</span> tree:</span><br><span class="line">        pc=td.xpath(<span class="string">'//div[@class="item_t"]/div[1]/a/img/@src'</span>)        <span class="comment">#图片url</span></span><br><span class="line">        name=td.xpath(<span class="string">'//div[@class="item_t"]/div[1]/span/text()'</span>)   <span class="comment">#名字</span></span><br><span class="line">   </span><br><span class="line">        <span class="comment"># print(pc)</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> pc:</span><br><span class="line">            pc_url=<span class="string">'http://www.xueshengmai.com'</span>+x             <span class="comment">#美女图片</span></span><br><span class="line">            <span class="comment"># print(pc_url)</span></span><br><span class="line">            <span class="comment"># img_data=response.content</span></span><br><span class="line">            res=requests.get(url=pc_url,headers=headers)</span><br><span class="line">            img_data=res.content</span><br><span class="line">       </span><br><span class="line">            girl=pc_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">            <span class="comment"># print(girl)</span></span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">'%s'</span>%girl,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(img_data)</span><br></pre></td></tr></table></figure>

<p>爬取梨视频</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  梨视频数据的爬取</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"><span class="comment"># 安装fake_useragent: pip install fake_useragent</span></span><br><span class="line">url = <span class="string">"https://www.pearvideo.com/category_8"</span></span><br><span class="line">ua = UserAgent().random</span><br><span class="line"><span class="comment"># 定制请求头</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 获取首页页面数据</span></span><br><span class="line">page_text = requests.get(url=url, headers=headers).text</span><br><span class="line"><span class="comment"># print("111", page_text)</span></span><br><span class="line"><span class="comment"># 获取首页页面数据的相关视频详情连接进行解析</span></span><br><span class="line">tree = etree.HTML(page_text)</span><br><span class="line">li_list = tree.xpath(<span class="string">"//div[@id='listvideoList']/ul/li"</span>)</span><br><span class="line">print(<span class="string">"哈哈哈"</span>, li_list)</span><br><span class="line"><span class="comment"># 详情的url</span></span><br><span class="line">detail_urls = []  <span class="comment"># type:list</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">    detail_url = <span class="string">"http://www.pearvideo.com/"</span> + li.xpath(<span class="string">"./div/a/@href"</span>)[<span class="number">0</span>]  <span class="comment"># 此时返回的是一个列表</span></span><br><span class="line">    print(<span class="string">"222"</span>,detail_url)</span><br><span class="line">    title = li.xpath(<span class="string">"./div/a/div[@class='vervideo-title']/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">    detail_urls.append(detail_url)</span><br><span class="line"></span><br><span class="line">print(detail_urls)</span><br><span class="line"><span class="comment"># 拿到每一个详情url发送get请求</span></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> detail_urls:</span><br><span class="line">    page_text = requests.get(url=url, headers=headers).text</span><br><span class="line">    video_url = re.findall(<span class="string">'srcUrl="(.*?)"'</span>, page_text, re.S)[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">"啧啧啧"</span>, video_url)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 向视频的详情发送请求</span></span><br><span class="line">    data = requests.get(url=video_url, headers=headers).content</span><br><span class="line">    fileName = str(random.randint(<span class="number">1</span>,<span class="number">10000</span>)) + <span class="string">'.mp4'</span>  <span class="comment"># 随机生成视频文件</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储得到本地</span></span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"wb"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(data)</span><br><span class="line">        print(fileName + <span class="string">"下载成功!"</span>)</span><br></pre></td></tr></table></figure>

<p>随手拍，分享经典一刻</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">url=<span class="string">'http://jandan.net/ooxx'</span></span><br><span class="line"></span><br><span class="line">headers=&#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">res=requests.get(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">tree=etree.HTML(res.text)</span><br><span class="line"></span><br><span class="line">td_list=tree.xpath(<span class="string">'//div[@class="row"]/div[2]'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> td <span class="keyword">in</span> td_list:</span><br><span class="line">    img_src=<span class="string">'http:'</span>+td.xpath(<span class="string">'./p/img/@src'</span>)[<span class="number">0</span>]    <span class="comment">#需要拼接url</span></span><br><span class="line">    img_data=requests.get(url=img_src).content   <span class="comment">#图片二进制流</span></span><br><span class="line">    img_name=img_src.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'%s'</span>%img_name,<span class="string">'wb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(img_data)</span><br></pre></td></tr></table></figure>

<h6 id="Selenium介绍"><a href="#Selenium介绍" class="headerlink" title="Selenium介绍"></a>Selenium介绍</h6><p>selenium是一个web自动化测试用的框架。程序员可以通过代码实现对浏览器的控制，比如打开网页，点击网页中的元素，实现鼠标滚动操作。<br>它支持多款浏览器，如谷歌浏览器，火狐浏览器等等，当然也支持无头浏览器。<br>目的：<br>    在爬取数据的过程中，经常遇到动态数据加载，一般动态数据加载有两种，一种通过ajax请求加载数据，另一种通过js代码加载动态数据。selenium可以模拟人操作真实浏览器，获取加载完成的页面数据。</p>
<p> ajax:<br>    url有规律且未加密，直接构建url连接请求<br>    url加密过无法破解规律  —–&gt;selenium<br>    js动态数据加载 —–&gt;  selenium</p>
<p>三元素：浏览器  驱动程序  selenium框架<br>    浏览器：推荐谷歌浏览器，标准稳定版本<br>    pip install selenium</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser=webdriver.Chrome(<span class="string">'./webdriver'</span>)  <span class="comment">#将驱动放在脚本所在的文件夹</span></span><br><span class="line"></span><br><span class="line">browser.get(<span class="string">'www.baidu.com'</span>)</span><br></pre></td></tr></table></figure>

<h6 id="selenium常用操作"><a href="#selenium常用操作" class="headerlink" title="selenium常用操作"></a>selenium常用操作</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实列化浏览器对象</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser=webdriver.Chrome(<span class="string">'driverpath'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#发送get请求</span></span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取页面元素</span></span><br><span class="line">find_element_by_id:根据元素的id</span><br><span class="line">find_element_by_name:根据元素的name</span><br><span class="line">find_element_by_xpath:根据xpath表达式</span><br><span class="line">find_element_by_class_name:根据<span class="class"><span class="keyword">class</span>的值</span></span><br><span class="line"><span class="class"><span class="title">find_element_by_css_selector</span>:</span>根据css选择器</span><br><span class="line"></span><br><span class="line"><span class="comment"># 节点交互操作:</span></span><br><span class="line">click(): 点击</span><br><span class="line">send_keys(): 输入内容</span><br><span class="line">clear(): 清空操作</span><br><span class="line">execute_script(js): 执行指定的js代码</span><br><span class="line"><span class="comment"># JS代码: window.scrollTo(0, document.body.scrollHeight)可以模拟鼠标滚动一屏高度</span></span><br><span class="line">quit(): 退出浏览器</span><br><span class="line">    I quit! 我不干!</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 获取网页的数据:</span></span><br><span class="line">browser.page_source   ---&gt;  str类型</span><br><span class="line"></span><br><span class="line"><span class="comment"># frame</span></span><br><span class="line">switch_to.frame(<span class="string">'frameid'</span>)</span><br></pre></td></tr></table></figure>

<p>selenium百度图片抓取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 百度图片抓取:</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.实例化浏览器对象</span></span><br><span class="line">browser = webdriver.Chrome(<span class="string">'./chromedriver.exe'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.向服务器发起请求</span></span><br><span class="line">browser.get(<span class="string">'http://image.baidu.com/'</span>)</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.输入关键字</span></span><br><span class="line">input_tag = browser.find_element_by_id(<span class="string">'kw'</span>)</span><br><span class="line">input_tag.send_keys(<span class="string">'腰子姐'</span>)</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.点击搜索</span></span><br><span class="line">button = browser.find_element_by_class_name(<span class="string">'s_search'</span>)</span><br><span class="line">button.click()</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.实现滚动下拉</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    browser.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight)'</span>)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">text = browser.page_source</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.实现数据解析</span></span><br><span class="line">soup = BeautifulSoup(text, <span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">li_list = soup.select(<span class="string">'.imgpage ul li'</span>)</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">    href = li[<span class="string">'data-objurl'</span>]</span><br><span class="line">    <span class="comment"># print(href)</span></span><br><span class="line">    <span class="comment"># request.urlretrieve(href, '%s.jpg'%li_list.index(li))</span></span><br><span class="line">    res = requests.get(url=href, headers=headers)</span><br><span class="line">    <span class="comment"># res.text: 文本数据</span></span><br><span class="line">    <span class="comment"># res.json(): json数据</span></span><br><span class="line">    <span class="comment"># res.content: 二进制流</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'%s.jpg'</span>%li_list.index(li), <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(res.content)</span><br><span class="line"></span><br><span class="line">browser.quit()</span><br></pre></td></tr></table></figure>





<p>qq空间模拟登陆</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 实例化浏览器对象</span></span><br><span class="line">browser = webdriver.Chrome(<span class="string">r'C:\Users\lenovo\Desktop\sp\chromedriver.exe'</span>)</span><br><span class="line"><span class="comment">#注意浏览器版本号与插件版本号</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开qq空间登陆页面</span></span><br><span class="line">browser.get(<span class="string">'https://qzone.qq.com/'</span>)</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 转至frame子页面</span></span><br><span class="line">browser.switch_to.frame(<span class="string">'login_frame'</span>)</span><br><span class="line"><span class="comment"># 获取密码登陆选项并点击</span></span><br><span class="line">a_tag = browser.find_element_by_id(<span class="string">'switcher_plogin'</span>)</span><br><span class="line">a_tag.click()</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 获取账号输入框并输入账号</span></span><br><span class="line">browser.find_element_by_id(<span class="string">'u'</span>).clear()</span><br><span class="line">user = browser.find_element_by_id(<span class="string">'u'</span>)</span><br><span class="line">user.send_keys(<span class="string">'账号'</span>)</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 获取密码输入框并输入密码</span></span><br><span class="line">browser.find_element_by_id(<span class="string">'p'</span>).clear()</span><br><span class="line">pwd = browser.find_element_by_id(<span class="string">'p'</span>)</span><br><span class="line">pwd.send_keys(<span class="string">'密码'</span>)</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 获取登陆按钮并单击</span></span><br><span class="line">button = browser.find_element_by_id(<span class="string">'login_button'</span>)</span><br><span class="line">button.click()</span><br></pre></td></tr></table></figure>

<h6 id="Scrapy安装与环境依赖"><a href="#Scrapy安装与环境依赖" class="headerlink" title="Scrapy安装与环境依赖"></a>Scrapy安装与环境依赖</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.在安装scrapy前需要安装好相应的依赖库, 再安装scrapy, 具体安装步骤如下:</span></span><br><span class="line">    (<span class="number">1</span>).安装lxml库: pip install lxml</span><br><span class="line">    (<span class="number">2</span>).安装wheel: pip install wheel</span><br><span class="line">    (<span class="number">3</span>).安装twisted: pip install twisted文件路径</span><br><span class="line">    	(twisted需下载后本地安装,下载地址:http://www.lfd.uci.edu/~gohlke/pythonlibs/<span class="comment">#twisted)</span></span><br><span class="line">    	(版本选择如下图,版本后面有解释,请根据自己实际选择)</span><br><span class="line">    (<span class="number">4</span>).安装pywin32: pip install pywin32</span><br><span class="line">    	(注意:以上安装步骤一定要确保每一步安装都成功,没有报错信息,如有报错自行百度解决)</span><br><span class="line">    (<span class="number">5</span>).安装scrapy: pip install scrapy</span><br><span class="line">    	(注意:以上安装步骤一定要确保每一步安装都成功,没有报错信息,如有报错自行百度解决)</span><br><span class="line">    (<span class="number">6</span>).成功验证:在cmd命令行输入scrapy,显示Scrapy1<span class="number">.6</span><span class="number">.0</span>-no active project,证明安装成功</span><br></pre></td></tr></table></figure>

<p>2.创建项目</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>手动创建一个目录test</span><br><span class="line"><span class="number">2.</span>在test文件夹下创建项目为：scrapy startproject spiderpro</span><br><span class="line"><span class="number">3.</span>进入项目文件夹： cd spiderpro</span><br><span class="line"><span class="number">4.</span>创建爬虫文件: Scrapy genspider 爬虫名  域名</span><br></pre></td></tr></table></figure>

<p>3.项目目录介绍</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">spiderpro</span><br><span class="line">　　spiderpro <span class="comment"># 项目目录</span></span><br><span class="line">　　　　__init__</span><br><span class="line">　　　　spiders:爬虫文件目录</span><br><span class="line">　　　　　　__init__</span><br><span class="line">　　　　　　tests.py:爬虫文件</span><br><span class="line">　　　　items.py:定义爬取数据持久化的数据结构</span><br><span class="line">　　　　middlewares.py:定义中间件</span><br><span class="line">　　　　pipelines.py:管道,持久化存储相关</span><br><span class="line">　　　　settings.py:配置文件</span><br><span class="line">　　venv:虚拟环境目录</span><br><span class="line">　 scrapy.cfg: scrapy项目配置文件</span><br></pre></td></tr></table></figure>

<p>说明</p>
<p>　(1).spiders:其内包含一个个Spider的实现, 每个Spider是一个单独的文件<br>　　(2).items.py:它定义了Item数据结构, 爬取到的数据存储为哪些字段<br>　　(3).pipelines.py:它定义Item Pipeline的实现<br>　　(4).settings.py:项目的全局配置<br>　　(5).middlewares.py:定义中间件, 包括爬虫中间件和下载中间件<br>　　(6).scrapy.cfg:它是scrapy项目的配置文件, 其内定义了项目的配置路径, 部署相关的信息等</p>
<hr>
<h6 id="Scrapy框架介绍：5大核心组件与数据流向"><a href="#Scrapy框架介绍：5大核心组件与数据流向" class="headerlink" title="Scrapy框架介绍：5大核心组件与数据流向"></a>Scrapy框架介绍：5大核心组件与数据流向</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>).架构:</span><br><span class="line"></span><br><span class="line">　　Scrapy Engine: 这是引擎，负责Spiders、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等等!</span><br><span class="line"></span><br><span class="line">　　Scheduler(调度器): 它负责接受引擎发送过来的requests请求，并按照一定的方式进行整理排列，入队、并等待Scrapy Engine(引擎)来请求时，交给引擎。</span><br><span class="line"></span><br><span class="line">　　Downloader（下载器)：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spiders来处理</span><br><span class="line"></span><br><span class="line">　　Spiders：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)，</span><br><span class="line"></span><br><span class="line">　　Item Pipeline：它负责处理Spiders中获取到的Item，并进行处理，比如去重，持久化存储（存数据库，写入文件，总之就是保存数据用的）</span><br><span class="line"></span><br><span class="line">　　Downloader Middlewares(下载中间件)：你可以当作是一个可以自定义扩展下载功能的组件</span><br><span class="line"></span><br><span class="line">　　Spider Middlewares(Spider中间件)：你可以理解为是一个可以自定扩展和操作引擎和Spiders中间‘通信‘的功能组件（比如进入Spiders的Responses;和从Spiders出去的Requests）</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">2</span>).工作流:</span><br><span class="line"></span><br><span class="line">　　<span class="number">1.</span>spider将请求发送给引擎, 引擎将request发送给调度器进行请求调度</span><br><span class="line"></span><br><span class="line">　　<span class="number">2.</span>调度器把接下来要请求的request发送给引擎, 引擎传递给下载器, 中间会途径下载中间件</span><br><span class="line"></span><br><span class="line">　　<span class="number">3.</span>下载携带request访问服务器, 并将爬取内容response返回给引擎, 引擎将response返回给spider</span><br><span class="line"></span><br><span class="line">　　<span class="number">4.</span>spider将response传递给自己的parse进行数据解析处理及构建item一系列的工作, 最后将item返回给引擎, 引擎传递个pipeline</span><br><span class="line"></span><br><span class="line">　　<span class="number">5.</span>pipeline获取到item后进行数据持久化</span><br><span class="line"></span><br><span class="line">　　<span class="number">6.</span>以上过程不断循环直至爬虫程序终止</span><br></pre></td></tr></table></figure>

<p>5.使用scrapy框架爬取糗百</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建项目</span></span><br><span class="line">scrapy startproject qbsk</span><br><span class="line">cd qsbk  <span class="comment">#切换到项目目录</span></span><br><span class="line">scrapy genspider qsbk_hot www.qiushibaike.com <span class="comment">#创建爬虫文件，qsbk_hot为爬虫名</span></span><br><span class="line">www....com为爬取范围</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#item文件定义数据存储的字段：</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()  <span class="comment"># 标题</span></span><br><span class="line">    lau = scrapy.Field()  <span class="comment"># 好笑数</span></span><br><span class="line">    comment = scrapy.Field()  <span class="comment"># 评论数</span></span><br><span class="line">    auth = scrapy.Field()  <span class="comment"># 作者</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spider文件中定义解析数据的方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkHotSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">	name =<span class="string">'qsbk_hot'</span></span><br><span class="line">	<span class="comment"># allowed_domains = ['www.qiushibaike.com'] # 无用, 可注释掉</span></span><br><span class="line">	start_urls =[<span class="string">'http://www.qiushibaike.com/'</span>]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 思路:一条热点数据在前端中对应一个li标签, 将一页中的所有li标签取出, 再进一步操作</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">		li_list = response.selector.xpath(<span class="string">'//div[@class="recommend-article"]/ul/li'</span>)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 循环li标签组成的列表, 先实例化item, 再取需要的字段, 并该item对象的相应属性赋值</span></span><br><span class="line">		<span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">			<span class="comment"># 实例化item对象</span></span><br><span class="line">			item =QsbkItem()</span><br><span class="line">			<span class="comment"># 解析获取title(标题), lau(好笑数), comment(评论数), auth(作者)等信息</span></span><br><span class="line">			title = ....</span><br><span class="line">			lau = ....</span><br><span class="line">			comment = ....</span><br><span class="line">			auth = ....</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 将字段的值存储在item的属性中</span></span><br><span class="line"></span><br><span class="line">			<span class="comment"># 返回item, 框架会自动将item传送至pipeline中的指定类</span></span><br><span class="line">			<span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在pipeline中定义管道类进行数据的存储</span></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line">classQsbkPipeline(object):</span><br><span class="line">　　<span class="comment"># 连接MongoDB数据库</span></span><br><span class="line">	conn = pymongo.MongoClient(<span class="string">"localhost"</span>, <span class="number">27017</span>)</span><br><span class="line">	db = conn.qiubai</span><br><span class="line">	table = db.qb_hot</span><br><span class="line"></span><br><span class="line">　　<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">　　　　<span class="comment"># 向数据库中出入数据</span></span><br><span class="line">　　　　self.table.insert(dict(item))</span><br><span class="line"></span><br><span class="line">　　　　<span class="comment"># 此处return item是为了下一个管道类能够接收到item进行存储</span></span><br><span class="line">　　　　<span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">　　<span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self)</span>:</span></span><br><span class="line">　　　　<span class="comment"># 关闭数据库连接</span></span><br><span class="line">　　　　self.conn.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此示例中配置文件中的配置的项, 注意是不是全部的配置, 是针对该项目增加或修改的配置项</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 忽略robots协议</span></span><br><span class="line">ROBOTSTXT_OBEY =<span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># UA伪装</span></span><br><span class="line">USER_AGENT = <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.87 Safari/537.36'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 管道类的注册配置</span></span><br><span class="line">ITEM_PIPELINES =&#123;</span><br><span class="line"><span class="string">'qsbk.pipelines.QsbkPipeline'</span>:<span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="Scrapy爬取校花网人名与图片下载链接"><a href="#Scrapy爬取校花网人名与图片下载链接" class="headerlink" title="Scrapy爬取校花网人名与图片下载链接"></a>Scrapy爬取校花网人名与图片下载链接</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需求: 爬取校花网大学校花的默认的第一页的所有图片src和人名, 并通过管道存入mongodb数据库</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建item类, 用于存储解析出的数据</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuaspiderItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    src = scrapy.Field()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spider中定义爬取的行为与解析数据的操作</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> XiaohuaspiderItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuaSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'hua'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['www.baidu.com']</span></span><br><span class="line">    start_urls = [<span class="string">'http://www.xiaohuar.com/hua/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        div_list = response.xpath(<span class="string">'//div[@class="img"]'</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            item = XiaohuaspiderItem()</span><br><span class="line">            name = ...(xpath匹配)</span><br><span class="line">            src = ...(xpath匹配)</span><br><span class="line">            <span class="comment"># 将数据存储到item的属性中</span></span><br><span class="line">            item[...] = ...</span><br><span class="line">            item[...] = ...</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># itemPipeline编码, 持久化数据到本地</span></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuaspiderPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    conn = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">    db = conn.xiaohua</span><br><span class="line">    table = db.hua</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.table.insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.conn.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置项:</span></span><br><span class="line"><span class="comment"># UA伪装:</span></span><br><span class="line">USER_AGENT = <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.87 Safari/537.36'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 忽略robots协议:</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启管道类</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'xiaohuaspider.pipelines.XiaohuaspiderPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.确定爬取的字段, 在items.py中定义字段</span></span><br><span class="line"><span class="comment"># 2.将要爬取的url, 放置在start_urls, 要把allowed_domains注释掉</span></span><br><span class="line"><span class="comment"># 3.在parse当中定义解析规则</span></span><br><span class="line">	<span class="number">1</span>).解析响应数据</span><br><span class="line">	<span class="number">2</span>).存储在临时的容器中, item对象</span><br><span class="line">	<span class="number">3</span>).<span class="keyword">yield</span> item: 将item提交给管道</span><br><span class="line"><span class="comment"># 4.在管道中实现与数据库的交互</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RtysPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        conn = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">        db = conn.rtys</span><br><span class="line">        table = db.ys</span><br><span class="line">        table.insert_one(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"><span class="comment"># 运行项目：</span></span><br><span class="line">scrapy crawl 爬虫名</span><br></pre></td></tr></table></figure>



<h6 id="1-Scrapy多页爬取"><a href="#1-Scrapy多页爬取" class="headerlink" title="1.Scrapy多页爬取"></a>1.Scrapy多页爬取</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spider编码在原基础之上, 构建其他页面的url地址, 并利用scrapy.Request发起新的请求, 请求的回调函数依然是parse:</span></span><br><span class="line">page = <span class="number">1</span></span><br><span class="line">base_url = <span class="string">'http://www.xiaohuar.com/list-1-%s.html'</span></span><br><span class="line"><span class="keyword">if</span> self.page &lt; <span class="number">4</span>:</span><br><span class="line">    page_url = base_url%self.page</span><br><span class="line">    self.page += <span class="number">1</span></span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(url=page_url, callback=self.parse)</span><br><span class="line"><span class="comment"># (其他文件不用改动)</span></span><br></pre></td></tr></table></figure>

<h6 id="2-scrapy爬取详情页"><a href="#2-scrapy爬取详情页" class="headerlink" title="2.scrapy爬取详情页"></a>2.scrapy爬取详情页</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需求: 爬取笑话的标题与详情页连接, 通过详情页链接, 爬取详情页笑话内容</span></span><br><span class="line"><span class="comment"># item编码: 定义数据持久化的字段信息</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JokeItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spider的编码:</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> JokeItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XhSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'xh'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['www.baidu.com']</span></span><br><span class="line">    start_urls = [<span class="string">'http://www.jokeji.cn/list.htm'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        li_list = response.xpath(<span class="string">'//div[@class="list_title"]/ul/li'</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            title = li.xpath(<span class="string">'./b/a/text()'</span>).extract_first()</span><br><span class="line">            link = <span class="string">'http://www.jokeji.cn'</span> + li.xpath(<span class="string">'./b/a/@href'</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=link, callback=self.datail_parse, meta=&#123;<span class="string">"title"</span>:title&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">datail_parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        joke_list = response.xpath(<span class="string">'//span[@id="text110"]//text()'</span>).extract()</span><br><span class="line">        title = response.meta[<span class="string">"title"</span>]</span><br><span class="line">        content = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> joke_list:</span><br><span class="line">            content += s</span><br><span class="line">        item = JokeItem()</span><br><span class="line">        item[<span class="string">"title"</span>] = title</span><br><span class="line">        item[<span class="string">"content"</span>] = content</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pipeline编码: 数据持久化具体操作</span></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JokePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    conn = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">    db = conn.haha</span><br><span class="line">    table = db.hahatable</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.table.insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.conn.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># settings配置编码:</span></span><br><span class="line">UA伪装</span><br><span class="line">Robots协议</span><br><span class="line">Item_Pipeline</span><br></pre></td></tr></table></figure>

<h6 id="3-scrapy发送post请求"><a href="#3-scrapy发送post请求" class="headerlink" title="3.scrapy发送post请求"></a>3.scrapy发送post请求</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'fy'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['www.baidu.com']</span></span><br><span class="line">    start_urls = [<span class="string">'https://fanyi.baidu.com/sug'</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'kw'</span>:<span class="string">'boy'</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">yield</span> scrapy.FormRequest(url=self.start_urls[<span class="number">0</span>], callback=self.parse, formdata=data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(<span class="number">1111111111111111111111111111111111111111111111111111111111111111111111111111111111</span>)</span><br><span class="line">        print(response.text)</span><br><span class="line">        print(json.loads(response.text))</span><br><span class="line">        </span><br><span class="line"> print(<span class="number">2222222222222222222222222222222222222222222222222222222222222222222222222222222222</span>)</span><br></pre></td></tr></table></figure>



<h6 id="scrapy对接selenium"><a href="#scrapy对接selenium" class="headerlink" title="scrapy对接selenium"></a>scrapy对接selenium</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">selenium可以实现抓取动态数据</span><br><span class="line">scrapy不能抓取动态数据, 如果是ajax请求, 可以请求接口, 如果是js动态加载, 需要结合selenium</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> WynewsItem</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ChromeOptions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NewsSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'news'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['www.baidu.com']</span></span><br><span class="line">    start_urls = [<span class="string">'https://news.163.com/domestic/'</span>]</span><br><span class="line">    option.add_experimental_option(<span class="string">'excludeSwitches'</span>, [<span class="string">'enable-automation'</span>])				      			bro=webdriver.Chrome(executable_path=<span class="string">r'C:\Users\Administrator\Desktop\news\wynews\wynews\spiders\chromedriver.exe'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail_parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        content_list = response.xpath(<span class="string">'//div[@id="endText"]/p//text()'</span>).extract()</span><br><span class="line">        content = <span class="string">''</span></span><br><span class="line">        title = response.meta[<span class="string">'title'</span>]</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> content_list:</span><br><span class="line">            content += s</span><br><span class="line">        item = WynewsItem()</span><br><span class="line">        item[<span class="string">"title"</span>] = title</span><br><span class="line">        item[<span class="string">"content"</span>] = content</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        div_list = response.xpath(<span class="string">'//div[contains(@class, "data_row")]'</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            link = div.xpath(<span class="string">'./a/@href'</span>).extract_first()</span><br><span class="line">            title = div.xpath(<span class="string">'./div/div[1]/h3/a/text()'</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=link, callback=self.detail_parse, meta=&#123;<span class="string">"title"</span>:title&#125;)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 中间件编码:</span></span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WynewsDownloaderMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span></span><br><span class="line">        bro = spider.bro</span><br><span class="line">        <span class="keyword">if</span> request.url <span class="keyword">in</span> spider.start_urls:</span><br><span class="line">            bro.get(request.url)</span><br><span class="line">            time.sleep(<span class="number">3</span>)</span><br><span class="line">            js = <span class="string">'window.scrollTo(0, document.body.scrollHeight)'</span></span><br><span class="line">            bro.execute_script(js)</span><br><span class="line">            time.sleep(<span class="number">3</span>)</span><br><span class="line">            response_selenium = bro.page_source</span><br><span class="line">            <span class="keyword">return</span> HtmlResponse(url=bro.current_url, body=response_selenium, encoding=<span class="string">"utf-8"</span>, request=request)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pipeline编码:</span></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WynewsPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    conn = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">    db = conn.wynews</span><br><span class="line">    table = db.newsinfo</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.table.insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<h6 id="pipeline数据持久化"><a href="#pipeline数据持久化" class="headerlink" title="pipeline数据持久化"></a>pipeline数据持久化</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MongoDB交互:</span></span><br><span class="line"><span class="keyword">import</span> Pymongo</span><br><span class="line"><span class="comment"># 管道类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化方法, __new__: 构造方法, 在内存中开辟一块空间</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">        	mongo_uri = crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db = crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.db[<span class="string">'news'</span>].insert(dict(item))</span><br><span class="line">        <span class="comment"># 在一个项目中可能存在多个管道类, 如果该管道类后面还有管道类需要存储数据, 必须return item</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MySQL交互:</span></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host, database, user, password, port)</span>:</span></span><br><span class="line">        self.host = host</span><br><span class="line">        self.database = database</span><br><span class="line">        self.user = user</span><br><span class="line">        self.password = password</span><br><span class="line">        self.port = port</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">        	host = crawler.settings.get(<span class="string">'MYSQL_HOST'</span>)</span><br><span class="line">            database = crawler.settings.get(<span class="string">'MYSQL_DATABASE'</span>)</span><br><span class="line">            user = crawler.settings.get(<span class="string">'MYSQL_USER'</span>)</span><br><span class="line">            password= crawler.settings.get(<span class="string">'MYSQL_PASSWORD'</span>)</span><br><span class="line">            port = crawler.settings.get(<span class="string">'MYSQL_PORT'</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.db = pymysql.connect(self.host, self.user, self.password, self.database, charset=<span class="string">'utf8'</span>, port=self.port)</span><br><span class="line">        self.cursor = self.db.cursor()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        data = dict(item)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># data.keys()--&gt; 获取所有的键, 字段 --&gt; 'title,content'</span></span><br><span class="line">        keys = <span class="string">','</span>.join(data.keys())</span><br><span class="line">        <span class="comment"># ['%s']*len(data)  --&gt; ['%s', '%s']</span></span><br><span class="line">        <span class="comment"># ','.join(['%s', '%s'])  --&gt;  '%s,%s'</span></span><br><span class="line">        values = <span class="string">','</span>.join([<span class="string">'%s'</span>]*len(data))</span><br><span class="line">        sql = <span class="string">'insert into %s (%s) values (%s)'</span> % (tablename, keys, values)</span><br><span class="line">        self.cursor.execute(sql, tuple(data.values()))</span><br><span class="line">        self.db.commit()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于文件下载的管道类</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spider编码:</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> XhxhItem</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XhSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'xh'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['www.baidu.com']</span></span><br><span class="line">    start_urls = [<span class="string">'http://www.521609.com/qingchunmeinv/'</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        li_list = response.xpath(<span class="string">'//div[@class="index_img list_center"]/ul/li'</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            item = XhxhItem()</span><br><span class="line">            link = li.xpath(<span class="string">'./a[1]/img/@src'</span>).extract_first()</span><br><span class="line">            item[<span class="string">'img_link'</span>] = <span class="string">'http://www.521609.com'</span> + link</span><br><span class="line">            print(item)</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># items编码:</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XhxhItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    img_link = scrapy.Field()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 管道编码:</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XhxhPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImgPipeLine</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=item[<span class="string">'img_link'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">        url = request.url</span><br><span class="line">        file_name = url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">return</span> file_name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># settings编码:</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'xhxh.pipelines.XhxhPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">'xhxh.pipelines.ImgPipeLine'</span>: <span class="number">301</span>,</span><br><span class="line">&#125;</span><br><span class="line">IMAGES_STORE = <span class="string">'./mvs'</span></span><br></pre></td></tr></table></figure>

<h6 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装:</span></span><br><span class="line">pip install virtualenvwrapper-win</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常用命令:</span></span><br><span class="line">mkvirtualenv envname  <span class="comment"># 创建虚拟环境并自动切换到该环境下</span></span><br><span class="line">workon envname  <span class="comment"># 切换到某虚拟环境下</span></span><br><span class="line">pip list / pip show / pip freeze / pip freeze -all</span><br><span class="line">rmvirtualenv envname  <span class="comment"># 删除虚拟环境</span></span><br><span class="line">deactivate  <span class="comment"># 退出虚拟环境</span></span><br><span class="line">lsvirtualenv  <span class="comment"># 列出所有常见的虚拟环境</span></span><br><span class="line">mkvirtualenv --python==C:\...\python.exe envname  <span class="comment"># 指定Python解释器创建虚拟环境</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同一项目环境</span></span><br><span class="line">pip freeze &gt; requirements.txt</span><br><span class="line">pip install -r C:\...\requirements.txt</span><br><span class="line">pip uninstall -r C:\...\requirements.txt</span><br></pre></td></tr></table></figure>

<h6 id="Scrapy-amp-Django项目"><a href="#Scrapy-amp-Django项目" class="headerlink" title="Scrapy &amp; Django项目"></a>Scrapy &amp; Django项目</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spider编写:</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> dl.items <span class="keyword">import</span> DlItem</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'p'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['www.baidu.com']</span></span><br><span class="line">    start_urls = [<span class="string">'https://www.kuaidaili.com/free/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># print(response)</span></span><br><span class="line">        tr_list = response.xpath(<span class="string">'//*[@id="list"]/table/tbody/tr'</span>)</span><br><span class="line">        <span class="comment"># print(tr_list)</span></span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> tr_list:</span><br><span class="line">            ip = tr.xpath(<span class="string">'./td[1]/text()'</span>).extract_first()</span><br><span class="line">            port = tr.xpath(<span class="string">'./td[2]/text()'</span>).extract_first()</span><br><span class="line">            typ = tr.xpath(<span class="string">'./td[3]/text()'</span>).extract_first()</span><br><span class="line">            protocal = tr.xpath(<span class="string">'./td[4]/text()'</span>).extract_first()</span><br><span class="line">            position = tr.xpath(<span class="string">'./td[5]/text()'</span>).extract_first()</span><br><span class="line">            <span class="comment"># print(ip, port, protocal, position)</span></span><br><span class="line">            item = DlItem()</span><br><span class="line">            item[<span class="string">'ip'</span>] = ip</span><br><span class="line">            item[<span class="string">'port'</span>] = port</span><br><span class="line">            item[<span class="string">'typ'</span>] = typ</span><br><span class="line">            item[<span class="string">'protocal'</span>] = protocal</span><br><span class="line">            item[<span class="string">'position'</span>] = position</span><br><span class="line">            print(item)</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># items编码</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DlItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    ip = scrapy.Field()</span><br><span class="line">    port = scrapy.Field()</span><br><span class="line">    typ = scrapy.Field()</span><br><span class="line">    protocal = scrapy.Field()</span><br><span class="line">    position = scrapy.Field()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Django项目创建与所有配置:</span></span><br><span class="line"><span class="number">1.</span>models创建:</span><br><span class="line"><span class="keyword">from</span> django.db <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create your models here.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Proxy</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    ip = models.CharField(max_length=<span class="number">50</span>)</span><br><span class="line">    port = models.CharField(max_length=<span class="number">50</span>)</span><br><span class="line">    typ = models.CharField(max_length=<span class="number">50</span>)</span><br><span class="line">    protocal = models.CharField(max_length=<span class="number">50</span>)</span><br><span class="line">    position = models.CharField(max_length=<span class="number">50</span>)</span><br><span class="line">    </span><br><span class="line"><span class="number">2.</span>在scrapy框架项目中嵌入django</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(os.path.dirname(os.path.abspath(<span class="string">'.'</span>)))</span><br><span class="line">os.environ[<span class="string">'DJANGO_SETTINGS_MODULE'</span>] = <span class="string">'proxyscan.settings'</span></span><br><span class="line"><span class="comment"># 手动初始化Django：</span></span><br><span class="line"><span class="keyword">import</span> django</span><br><span class="line">django.setup()</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span>修改爬虫item:</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy_djangoitem <span class="keyword">import</span> DjangoItem</span><br><span class="line"><span class="keyword">from</span> proxy <span class="keyword">import</span> models</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DlItem</span><span class="params">(DjangoItem)</span>:</span></span><br><span class="line">    django_model = models.Proxy</span><br><span class="line">    </span><br><span class="line"><span class="number">4.</span>pipeline编码:</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DlPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'开启数据库, 进行数据存储'</span>)</span><br><span class="line">        item.save()</span><br><span class="line">        print(<span class="string">'关闭数据库'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line"><span class="number">5.</span>Django项目迁移数据库与admin后台配置</span><br><span class="line">Python manage.py makemigrations</span><br><span class="line">python manage.py migrate</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> proxy.models <span class="keyword">import</span> Proxy</span><br><span class="line">admin.site.register(Proxy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建超级用户:</span></span><br><span class="line">Python manage.py createsuperuser</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 路由:</span></span><br><span class="line"><span class="keyword">from</span> django.conf.urls <span class="keyword">import</span> url</span><br><span class="line"><span class="keyword">from</span> django.contrib <span class="keyword">import</span> admin</span><br><span class="line"><span class="keyword">from</span> proxy.views <span class="keyword">import</span> index</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    url(<span class="string">r'^admin/'</span>, admin.site.urls),</span><br><span class="line">    url(<span class="string">r'^index/'</span>, index),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 视图函数:</span></span><br><span class="line"><span class="keyword">from</span> django.shortcuts <span class="keyword">import</span> render</span><br><span class="line"><span class="keyword">from</span> proxy.models <span class="keyword">import</span> Proxy</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">(requests)</span>:</span></span><br><span class="line">    p = Proxy.objects.all()</span><br><span class="line">    <span class="keyword">return</span> render(requests, <span class="string">'index.html'</span>, &#123;<span class="string">"p"</span>:p&#125;)</span><br></pre></td></tr></table></figure>

<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前端代码:</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html lang=<span class="string">"en"</span>&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset=<span class="string">"UTF-8"</span>&gt;</span><br><span class="line">    &lt;title&gt;Title&lt;/title&gt;</span><br><span class="line">    &lt;script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"&gt;&lt;/script&gt;</span><br><span class="line">    &lt;link href=<span class="string">"https://cdn.bootcss.com/twitter-bootstrap/4.3.1/css/bootstrap.min.css"</span> rel=<span class="string">"stylesheet"</span>&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;div class="container"&gt;</span><br><span class="line">    &lt;div class="row" &gt;</span><br><span class="line">        &lt;div class="col-md-10 col-md-offset-2" style="margin:0 auto"&gt;</span><br><span class="line">            &lt;div class="panel panel-primary"&gt;</span><br><span class="line">                &lt;div class="panel-heading" style="margin-top:50px"&gt;</span><br><span class="line">                    &lt;h3 class="panel-title"&gt;代理IP一览表&lt;/h3&gt;</span><br><span class="line">                &lt;/div&gt;</span><br><span class="line">                &lt;div class="panel-body"&gt;</span><br><span class="line">                    &lt;table class="table table-striped"&gt;</span><br><span class="line">                        &lt;thead&gt;</span><br><span class="line">                        &lt;tr&gt;</span><br><span class="line">                            &lt;th&gt;IP&lt;/th&gt;</span><br><span class="line">                            &lt;th&gt;Port&lt;/th&gt;</span><br><span class="line">                            &lt;th&gt;Type&lt;/th&gt;</span><br><span class="line">                            &lt;th&gt;Protocal&lt;/th&gt;</span><br><span class="line">                            &lt;th&gt;Positon&lt;/th&gt;</span><br><span class="line">                        &lt;/tr&gt;</span><br><span class="line">                        &lt;/thead&gt;</span><br><span class="line">                        &lt;tbody&gt;</span><br><span class="line">                        &#123;% <span class="keyword">for</span> i <span class="keyword">in</span> p %&#125;</span><br><span class="line">                            &lt;tr&gt;</span><br><span class="line">                                &lt;th&gt;&#123;&#123; i.ip &#125;&#125;&lt;/th&gt;</span><br><span class="line">                                &lt;td&gt;&#123;&#123; i.port &#125;&#125;&lt;/td&gt;</span><br><span class="line">                                &lt;td&gt;&#123;&#123; i.typ &#125;&#125;&lt;/td&gt;</span><br><span class="line">                                &lt;td&gt;&#123;&#123; i.protocal &#125;&#125;&lt;/td&gt;</span><br><span class="line">                                &lt;td&gt;&#123;&#123; i.position &#125;&#125;&lt;/td&gt;</span><br><span class="line">                            &lt;/tr&gt;</span><br><span class="line">                        &#123;% endfor %&#125;</span><br><span class="line">                        &lt;/tbody&gt;</span><br><span class="line">                    &lt;/table&gt;</span><br><span class="line">                &lt;/div&gt;</span><br><span class="line">            &lt;/div&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>


    </div>

    
    
    
    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      
    </div>
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>杜家乐
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://luckyle.top/Spider/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B9%8BRequests%E6%A8%A1%E5%9D%97/" title="Python网络爬虫之Requests模块">https://luckyle.top/Spider/Python网络爬虫之Requests模块/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Spider/" rel="tag"># Spider</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/uncategorized/gitbook%E4%BD%BF%E7%94%A8/" rel="prev" title="gitbook使用">
      <i class="fa fa-chevron-left"></i> gitbook使用
    </a></div>
      <div class="post-nav-item">
    <a href="/uncategorized/Redis%E7%9F%A5%E8%AF%86%E7%82%B9/" rel="next" title="Redis知识点">
      Redis知识点 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  
  </div>
  

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Python网络爬虫之requests模块"><span class="nav-text">Python网络爬虫之requests模块</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#什么是requests模块"><span class="nav-text">什么是requests模块</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#为什么要使用request模块"><span class="nav-text">为什么要使用request模块</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#如何使用request模块"><span class="nav-text">如何使用request模块</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#request发送请求"><span class="nav-text">request发送请求</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#requests高阶用法"><span class="nav-text">requests高阶用法</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#xpath解析库"><span class="nav-text">xpath解析库</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#BeautifulSoup解析"><span class="nav-text">BeautifulSoup解析</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#代码展示"><span class="nav-text">代码展示</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#请求载体身份标识的伪装"><span class="nav-text">请求载体身份标识的伪装</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Selenium介绍"><span class="nav-text">Selenium介绍</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#selenium常用操作"><span class="nav-text">selenium常用操作</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Scrapy安装与环境依赖"><span class="nav-text">Scrapy安装与环境依赖</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Scrapy框架介绍：5大核心组件与数据流向"><span class="nav-text">Scrapy框架介绍：5大核心组件与数据流向</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Scrapy爬取校花网人名与图片下载链接"><span class="nav-text">Scrapy爬取校花网人名与图片下载链接</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-Scrapy多页爬取"><span class="nav-text">1.Scrapy多页爬取</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-scrapy爬取详情页"><span class="nav-text">2.scrapy爬取详情页</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-scrapy发送post请求"><span class="nav-text">3.scrapy发送post请求</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#scrapy对接selenium"><span class="nav-text">scrapy对接selenium</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#pipeline数据持久化"><span class="nav-text">pipeline数据持久化</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#虚拟环境"><span class="nav-text">虚拟环境</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Scrapy-amp-Django项目"><span class="nav-text">Scrapy &amp; Django项目</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="杜家乐"
      src="/images/tx.jpg">
  <p class="site-author-name" itemprop="name">杜家乐</p>
  <div class="site-description" itemprop="description">Progress is not created by contented people..</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">37</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/1315532054" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;1315532054" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i>RSS</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://47.97.210.11/" title="http:&#x2F;&#x2F;47.97.210.11&#x2F;" rel="noopener" target="_blank">基于Django搭建博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://changpengfei0.github.io/" title="https:&#x2F;&#x2F;changpengfei0.github.io&#x2F;" rel="noopener" target="_blank">飞弟</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://v3u.cn/" title="https:&#x2F;&#x2F;v3u.cn&#x2F;" rel="noopener" target="_blank">刘悦</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/xiaonq/" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;xiaonq&#x2F;" rel="noopener" target="_blank">萧乃强</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://v3u.cn/book/redis.html/" title="https:&#x2F;&#x2F;v3u.cn&#x2F;book&#x2F;redis.html&#x2F;" rel="noopener" target="_blank">interview</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/xinzaiyuan/" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;xinzaiyuan&#x2F;" rel="noopener" target="_blank">刘国鑫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://edu.csdn.net/notebook/python/" title="https:&#x2F;&#x2F;edu.csdn.net&#x2F;notebook&#x2F;python&#x2F;" rel="noopener" target="_blank">Python</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/weixin_44685869/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_44685869&#x2F;" rel="noopener" target="_blank">张汇森</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">
  
  &copy; 在很久很久以前 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>



  <span class="author" itemprop="copyrightHolder">杜家乐</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">138k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:05</span>
</div>



<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
    有<span id="busuanzi_value_site_uv"></span>人看过我的博客啦
</span>
</div>
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共59.8k字</span>
</div>


        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,0' opacity='0.5' zIndex='-1' count='150' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
<script src="/js/utils.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>












  

  


  <script type ="text/javascript" src ="/js/love-click.js"></script> 
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":300,"height":440},"mobile":{"show":false},"react":{"opacityDefault":1,"opacityOnHover":1},"log":false,"tagMode":false});</script></body>
</html>
